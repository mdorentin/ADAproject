{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import yfinance as yf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from itertools import product\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "seed = 1999\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ready_data(dataname='eurusd_tech', n_past=5, val_size=252, test_size=30):\n",
    "    '''\n",
    "    This function imports the data given the dataname given as a parameter and will then modify the dataframe\n",
    "    to something that the Machine Learning model can understand. Other parameters are avaialble:\n",
    "        * n_past: how much past data should the prediction be based on.\n",
    "        * validation_size: how much portion of the dataframe should the model be validated on.\n",
    "        * testing_size: on how much portion of the dataframe should the model be tested on.\n",
    "    \n",
    "    Output:\n",
    "        * scaler, X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    '''\n",
    "    typedata = dataname.split('_')[1]\n",
    "\n",
    "    if typedata == 'tech':\n",
    "        dataframe = pd.read_csv(f'data/technical_df/{dataname}.csv').iloc[:,1:]\n",
    "    elif typedata == 'macro':\n",
    "        dataframe = pd.read_csv(f'data/macro_df/{dataname}.csv').iloc[:,1:]\n",
    "    elif typedata == 'both':\n",
    "        dataframe = pd.read_csv(f'data/both_df/{dataname}.csv').iloc[:,1:]\n",
    "    else:\n",
    "        raise ValueError('Check the dataname.')\n",
    "\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    train_size = len(dataset) - (test_size + val_size)\n",
    "\n",
    "    test_df = dataset[-test_size:]\n",
    "    val_df = dataset[train_size:train_size+val_size]\n",
    "    train_df = dataset[:train_size]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(train_df)\n",
    "\n",
    "    train_df_scaled = scaler.fit_transform(train_df)\n",
    "    val_df_scaled = scaler.fit_transform(val_df)\n",
    "    test_df_scaled = scaler.fit_transform(test_df)\n",
    "\n",
    "    def df_to_X_y(dataset, n_past=5):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-n_past-1):\n",
    "            a = dataset[i:(i+n_past), :]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + n_past, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    X_train, y_train = df_to_X_y(train_df_scaled)\n",
    "    X_val, y_val = df_to_X_y(val_df_scaled)\n",
    "    X_test, y_test = df_to_X_y(test_df_scaled)\n",
    "\n",
    "    return scaler, X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def descaled(arr):\n",
    "    '''\n",
    "    This function descales the scaled data in order to have the real values.\n",
    "    '''\n",
    "    extended = np.zeros((len(arr), X_train.shape[2]))\n",
    "    extended[:, 0] = arr\n",
    "    return scaler.inverse_transform(extended)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_model(dataname, epochs_list, batch_size_list, learning_rate_list, units_list, units2_list, activation1_list, activation2_list):\n",
    "    model = 'RNN'\n",
    "    typedata = dataname.split('_')[1]\n",
    "    \n",
    "    total_combinations = len(epochs_list) * len(batch_size_list) * len(learning_rate_list) * len(units_list) * len(units2_list) * len(activation1_list) * len(activation2_list)\n",
    "    it = 1\n",
    "    results = []\n",
    "\n",
    "    scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "    for epochs, batch_size, learning_rate, units, units2, activation1, activation2 in product(epochs_list, batch_size_list, \n",
    "                                                                                     learning_rate_list, units_list, units2_list,\n",
    "                                                                                     activation1_list, activation2_list):\n",
    "        print(f'Iteration {it}/{total_combinations}:')\n",
    "        model_rnn = Sequential()\n",
    "        model_rnn.add(SimpleRNN(units, activation=activation1, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model_rnn.add(SimpleRNN(units2, activation=activation2, return_sequences=False))\n",
    "        model_rnn.add(Dense(1))\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model_rnn.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "        history_rnn = model_rnn.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0)\n",
    "\n",
    "        y_pred_rnn = model_rnn.predict(X_test)\n",
    "        y_pred_rnn = np.delete(descaled(y_pred_rnn.flatten()),0)\n",
    "        y_test_rnn = np.delete(descaled(y_test),-1)\n",
    "\n",
    "        mse = mean_squared_error(y_test_rnn, y_pred_rnn)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test_rnn, y_pred_rnn)\n",
    "\n",
    "        results.append({\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'units': units,\n",
    "            'units2': units2,\n",
    "            'activation1': activation1,\n",
    "            'activation2': activation2,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        })\n",
    "\n",
    "        it += 1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list=[50, 100]\n",
    "batch_size_list=[16, 32, 64]\n",
    "learning_rate_list=[0.001, 0.01, 0.1]\n",
    "units_list=[16, 32, 64]\n",
    "units2_list=[16, 32, 64]\n",
    "activation1_list=['relu', 'sigmoid', 'tanh']\n",
    "activation2_list=['relu', 'sigmoid', 'tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'eurusd_tech'\n",
    "scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "tech_rnn = RNN_model(\n",
    "    dataname,\n",
    "    epochs_list,\n",
    "    batch_size_list,\n",
    "    learning_rate_list,\n",
    "    units_list,\n",
    "    units2_list,\n",
    "    activation1_list,\n",
    "    activation2_list)\n",
    "\n",
    "tech_rnn_df = pd.DataFrame(tech_rnn).sort_values(by='mse')\n",
    "tech_rnn_df.to_csv('data/hyper_parameters/tech_rnn_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'eurusd_macro'\n",
    "scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "macro_rnn = RNN_model(\n",
    "    dataname,\n",
    "    epochs_list,\n",
    "    batch_size_list,\n",
    "    learning_rate_list,\n",
    "    units_list,\n",
    "    units2_list,\n",
    "    activation1_list,\n",
    "    activation2_list)\n",
    "\n",
    "macro_rnn_df = pd.DataFrame(macro_rnn).sort_values(by='mse')\n",
    "macro_rnn_df.to_csv('data/hyper_parameters/macro_rnn_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'eurusd_both'\n",
    "scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "both_rnn = RNN_model(\n",
    "    dataname,\n",
    "    epochs_list,\n",
    "    batch_size_list,\n",
    "    learning_rate_list,\n",
    "    units_list,\n",
    "    units2_list,\n",
    "    activation1_list,\n",
    "    activation2_list)\n",
    "\n",
    "both_rnn_df = pd.DataFrame(both_rnn).sort_values(by='mse')\n",
    "both_rnn_df.to_csv('data/hyper_parameters/both_rnn_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(dataname, epochs_list, batch_size_list, learning_rate_list, units_list, units2_list, activation1_list, activation2_list):\n",
    "    model = 'LSTM'\n",
    "    typedata = dataname.split('_')[1]\n",
    "    \n",
    "    total_combinations = len(epochs_list) * len(batch_size_list) * len(learning_rate_list) * len(units_list) * len(units2_list) * len(activation1_list) * len(activation2_list)\n",
    "    it = 1\n",
    "    results = []\n",
    "\n",
    "    scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "    for epochs, batch_size, learning_rate, units, units2, activation1, activation2 in product(epochs_list, batch_size_list, \n",
    "                                                                                     learning_rate_list, units_list, units2_list,\n",
    "                                                                                     activation1_list, activation2_list):\n",
    "        print(f'Iteration {it}/{total_combinations}:')\n",
    "        model_lstm = Sequential()\n",
    "        model_lstm.add(LSTM(units, activation=activation1, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model_lstm.add(LSTM(units2, activation=activation2, return_sequences=False))\n",
    "        model_lstm.add(Dense(1))\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model_lstm.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "        history_lstm = model_lstm.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0)\n",
    "\n",
    "        y_pred_lstm = model_lstm.predict(X_test)\n",
    "        y_pred_lstm = np.delete(descaled(y_pred_lstm.flatten()),0)\n",
    "        y_test_lstm = np.delete(descaled(y_test),-1)\n",
    "\n",
    "        mse = mean_squared_error(y_test_lstm, y_pred_lstm)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test_lstm, y_pred_lstm)\n",
    "\n",
    "        results.append({\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'units': units,\n",
    "            'units2': units2,\n",
    "            'activation1': activation1,\n",
    "            'activation2': activation2,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        })\n",
    "\n",
    "        it += 1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'eurusd_tech'\n",
    "scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "tech_lstm = LSTM_model(\n",
    "    dataname,\n",
    "    epochs_list,\n",
    "    batch_size_list,\n",
    "    learning_rate_list,\n",
    "    units_list,\n",
    "    units2_list,\n",
    "    activation1_list,\n",
    "    activation2_list)\n",
    "\n",
    "tech_lstm_df = pd.DataFrame(tech_lstm).sort_values(by='mse')\n",
    "tech_lstm_df.to_csv('data/hyper_parameters/tech_lstm_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'eurusd_macro'\n",
    "scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "macro_lstm = LSTM_model(\n",
    "    dataname,\n",
    "    epochs_list,\n",
    "    batch_size_list,\n",
    "    learning_rate_list,\n",
    "    units_list,\n",
    "    units2_list,\n",
    "    activation1_list,\n",
    "    activation2_list)\n",
    "\n",
    "macro_lstm_df = pd.DataFrame(macro_lstm).sort_values(by='mse')\n",
    "macro_lstm_df.to_csv('data/hyper_parameters/macro_lstm_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'eurusd_both'\n",
    "scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "both_lstm = LSTM_model(\n",
    "    dataname,\n",
    "    epochs_list,\n",
    "    batch_size_list,\n",
    "    learning_rate_list,\n",
    "    units_list,\n",
    "    units2_list,\n",
    "    activation1_list,\n",
    "    activation2_list)\n",
    "\n",
    "both_lstm_df = pd.DataFrame(both_lstm).sort_values(by='mse')\n",
    "both_lstm_df.to_csv('data/hyper_parameters/both_lstm_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_model(dataname, epochs_list, batch_size_list, learning_rate_list, units_list, units2_list, activation1_list, activation2_list):\n",
    "    model = 'GRU'\n",
    "    typedata = dataname.split('_')[1]\n",
    "    \n",
    "    total_combinations = len(epochs_list) * len(batch_size_list) * len(learning_rate_list) * len(units_list) * len(units2_list) * len(activation1_list) * len(activation2_list)\n",
    "    it = 1\n",
    "    results = []\n",
    "\n",
    "    scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "    for epochs, batch_size, learning_rate, units, units2, activation1, activation2 in product(epochs_list, batch_size_list, \n",
    "                                                                                     learning_rate_list, units_list, units2_list,\n",
    "                                                                                     activation1_list, activation2_list):\n",
    "        print(f'Iteration {it}/{total_combinations}:')\n",
    "        model_gru = Sequential()\n",
    "        model_gru.add(GRU(units, activation=activation1, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model_gru.add(GRU(units2, activation=activation2, return_sequences=False))\n",
    "        model_gru.add(Dense(1))\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model_gru.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "        history_gru = model_gru.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0)\n",
    "\n",
    "        y_pred_gru = model_gru.predict(X_test)\n",
    "        y_pred_gru = np.delete(descaled(y_pred_gru.flatten()),0)\n",
    "        y_test_gru = np.delete(descaled(y_test),-1)\n",
    "\n",
    "        mse = mean_squared_error(y_test_gru, y_pred_gru)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test_gru, y_pred_gru)\n",
    "\n",
    "        results.append({\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'units': units,\n",
    "            'units2': units2,\n",
    "            'activation1': activation1,\n",
    "            'activation2': activation2,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        })\n",
    "\n",
    "        it += 1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'eurusd_tech'\n",
    "scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "tech_gru = GRU_model(\n",
    "    dataname,\n",
    "    epochs_list,\n",
    "    batch_size_list,\n",
    "    learning_rate_list,\n",
    "    units_list,\n",
    "    units2_list,\n",
    "    activation1_list,\n",
    "    activation2_list)\n",
    "\n",
    "tech_gru_df = pd.DataFrame(tech_gru).sort_values(by='mse')\n",
    "tech_gru_df.to_csv('data/hyper_parameters/tech_gru_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'eurusd_macro'\n",
    "scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "macro_gru = GRU_model(\n",
    "    dataname,\n",
    "    epochs_list,\n",
    "    batch_size_list,\n",
    "    learning_rate_list,\n",
    "    units_list,\n",
    "    units2_list,\n",
    "    activation1_list,\n",
    "    activation2_list)\n",
    "\n",
    "macro_gru_df = pd.DataFrame(macro_gru).sort_values(by='mse')\n",
    "macro_gru_df.to_csv('data/hyper_parameters/macro_gru_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'eurusd_both'\n",
    "scaler, X_train, y_train, X_val, y_val, X_test, y_test = get_ready_data(dataname)\n",
    "\n",
    "both_gru = GRU_model(\n",
    "    dataname,\n",
    "    epochs_list,\n",
    "    batch_size_list,\n",
    "    learning_rate_list,\n",
    "    units_list,\n",
    "    units2_list,\n",
    "    activation1_list,\n",
    "    activation2_list)\n",
    "\n",
    "both_gru_df = pd.DataFrame(both_gru).sort_values(by='mse')\n",
    "both_gru_df.to_csv('data/hyper_parameters/both_gru_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
